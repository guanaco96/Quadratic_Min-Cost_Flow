\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{geometry}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{xcolor}

\DeclareMathOperator*{\argmin}{arg\,min}
\geometry{a4paper, left=30mm, right=30mm}

\setlength\parindent{0pt}
\newcommand{\R}{\mathbbm{R}}
\newcommand{\1}{\mathbbm{1}}
\newcommand\note[1]{\textcolor{red}{\textbf{#1}}}
\newtheorem{theorem}{Theorem}


\title{Conjugate Gradient for \\ Quadratic Minimum-Cost Flow}
\author{Lorenzo Beretta, \texttt{loribere@gmail.com}
  \and Project NoML-13 of C.M. course, C.S. department, UniPi}
\date{3rd July 2019}

\begin{document}
\maketitle


\section{Introduction}
Let $D \in {\R^{m \times m}$ be a diagonal positive definite matrix and $E \in \R^{n \times m}$ be the node-edge matrix of a directed graph, meaning that, given an enumeration $e[1\dots m]$ of edges, we have
  \begin{equation*}
    E_{i, j} = \left\{
      \begin{align*}
        1 \qquad&  \text{ if $e[j] = (u, v)$ and $j = v$} \\
        -1 \qquad& \text{ if $e[j] = (u, v)$ and $j = u$} \\
        0 \qquad&  \text{ otherwise}
      \end{align*}
   \end{equation*}
        
The aim of this project is to exploit the Conjugate Gradient method to solve the
linear system

\begin{equation} \label{linear_system}
  \left(E D^{-1} E^t\right) x = b
\end{equation}

for some $b \in \R^n$. 

This problem arises from the KKT conditions of a quadratic separable Minimum-Cost
Flow problem, however we will neglect its origin and just deal with the linear system.


\section{Conjugate Gradient Algorithm}
In this section we will show the conjugate gradient method to solve a linear  system of the form $ A x = b $ for a symmetric and positive semidefinite matrix $A$.

Our main reference for this method is the book of Trefethen {\em et al.} \cite{trefethen97} that treats the strictly positive definite case, it is easy to adapt their proofs to show that assuming $b \in range(A)$ all results stated there hold even in the semidefinite case. This is necessary to our application since $E^t D^{-1} E$ has rank exactly $n-1$ in fact $rk(E)=n-c$ where $c$ is the number of connected components of the graph (assumed to be connected) and $rk(D) = m$.
Of course the algorithm converges to one of the infinite solutions of the system if $rk(A) < n$ and in particular it converges to $x_*$ such that $\1^t x_* = 0$, indeed $Ker(E^t D^{-1} E) = \langle \1 \rangle$.

\subsection{Algorithm}
In the following algorithm $x_n$ is the approximate solution at step $n$ and the cycle is actually repeated until a suitable convergence error is achieved\footnote{In our implementation we emulated the scipy's CG criterion, stopping as soon as $\frac{||r_n||_2}{ ||b||_2}$ falls below a fixed threshold.}.

\vbox{
\begin{algorithmic}[1]
\Procedure{ConjugateGradient}{$A, b$}
  \State {$ x_0 = 0, \quad r_0 = b, \quad p_0 = b $}
  \For {$ n = 1,\, 2,\, 3,\, \dots $}
    \State {$ \alpha_n = \left(r_ {n-1}^tr_ {n-1}\right) / \left(p_ {n-1}^tAp_ {n-1}}\right) $} \Comment{Step Length}
    \State {$ x_n = x_{n-1} + \alpha_n p_{n-1} $} \Comment{Approximate Solution}
    \State {$ r_n = r_{n-1} - \alpha_n A p_{n-1} $} \Comment{Residual}
    \State {$\beta_n = \left(r_n^t r_n\right) / \left(r_{n-1}^t r_{n-1}^t\right)$} \Comment{Improvement this Step}
    \State {$p_n = r_n + \beta_n p_{n-1} $} \Comment{Search Direction}
  \EndFor
\EndProcedure
\end{algorithmic}
}
\hspace{1cm}

It is worth noting that the bottleneck of the single iteration is the computation of $Ap_{n-1}$ and it can be fasten up considering the structure of A. It suffices to notice that $E$ has exactly $2m$ nonzero elements and employ a sparse matrix multiplication to achieve an $O(m)$ complexity per iteration. Moreover our implementation performs exactly $m$ multiplications and $3m$ additions keeping the constant factor very low\footnote{See \texttt{make\_operator} method in \texttt{cg.py} for more details.}.

\subsection{CG as a Direct Method}
The main property of this algorithm is that for every $n$ such that the algorithm has not converged yet it holds that:

\begin{equation*}
  \mathcal{K}_n = \langle x_1,\, \dots,\,x_n\rangle = \langle r_0,\, \dots,\,r_{n-1}\rangle = \langle p_0,\, \dots,\,p_{n-1}\rangle = \langle b,\,A b, \, \dots,\,A^{n-1} b\rangle \\
\end{equation*}

\begin{equation*}
  r_n^t r_j = 0, \quad p_n^t A p_j = 0 \quad \forall j < n
\end{equation*}
\hspace{0.5cm}

It is easy to show that this property entails that the algorithm is well defined even in the singular case since $ b \in range(A) \implies p_{n-1} \in range(A) \implies p_{n-1}^t A p_{n-1} \neq 0$. Moreover the fundamental corollary of that property is that  at each step
$$x_n = \argmin_{z \in \mathcal{K}_n} || x_* - z ||_A $$
for every $x_*$ such that $Ax_* = b$. Therefore in exact arithmetic it must converge in at most $n$ steps and the method is then a direct one.

\subsection{CG as an Interative Method}
Conjugate gradient has a twofold nature and can be interpreted as an optimization algorithm whose objective function is $ || x_* - x ||_A $ that up to a positive scaling and a shift is equal to $ \frac{1}{2}x^t A x - x^t b$. Then it becomes interesting to study its convergence rate trying to speedup the direct method bound of $n$ iterations.

The main result employed to derive the convergence rate is this slight modification of theorem 38.3 of Trefethen {\em et al.} \cite{trefethen97}:

\begin{theorem}[Polynomial Approximation]\label{polap}
  Let us define $e_n = x_* - x_n$ and $P_n = \left\{p \in \R[x]\, |\, deg(p) \leq n, \; p(0) = 1 \right\}$. If CG is not converged yet before step $n$ (i.e. $r_{n-1} \neq 0$) then 
$$ \frac{||e_n||_A}{||e_0||_A} \leq \inf_{p \in P_n}\frac{||p(A) e_0||_A}{||e_0||_A} \leq \inf_{p \in P_n} \max_{\lambda \in sp(A)\setminus\{0\}}|p(\lambda)|
$$
\end{theorem}
\begin{proof}
  Let us decompose $e_0$ over the orthonormal basis of $A$-eigenvectors as\\ $e_0 = \sum_i a_i v_i$ provided that $A v_i = \lambda_i v_i$, then
$$\inf_{p \in P_n}\frac{||p(A) e_0||_A}{||e_0||_A} \leq \inf_{p \in P_n}\sqrt{\frac{\sum_i a_i^2 \lambda_i \left(p(\lambda_i)\right)^2}{\sum_i a_i^2 \lambda_i}}\leq \inf_{p \in P_n} \max_{\lambda \in sp(A)\setminus\{0\}}|p(\lambda)|$$

\end{proof}

 Theorem \ref{polap} enhances theorem 38.3 of Trefethen {\em et al.} \cite{trefethen97} taking into account the singular case and showing that even in that case this bound doesn't degrade.

\subsection{Rate of Convergence}
The results above provide a better although trivial estimate for the ``exact method'' convergence time, in fact if $A$ has $h$ nonzero different eigenvalues then after $h$ iterations RHS in the previous inequality is null since the polynomial perfectly interpolates them.

On the other hand a more technical result involving the iterative nature of the algorithm can be proven: given $A$'s condition number $\kappa = \frac{\lambda_{max}}{\lambda_{min}}$, where $\lambda_{max}$ and $\lambda_{min}$ are extremal eigenvalues, it holds that

$$ \frac{||e_n||_A}{||e_0||_A} \leq 2 \left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^n $$

Finally we may say that a ``good'' spectrum for $A$ (i.e. providing fast convergence of CG) is one that has at least one of the two: (i) eigenvalue forms small clusters so that polynomials can interpolate them ``all at once'' or (ii) eigenvalues are not too separated in logarithmic scale. We will assess a strong dependence of convergence time on eigenvalues distribution experimentally. 


\section{Experiments}
\subsection{Data Generation}
In order to perform the experiments we needed to generate three objects: the matrix $E\in \R^{n\times m}$ (equivalent to generate a digraph's topology), the diagonal positive definite matrix $D\in\R^{m\times m$ and the vector $b\in\R^n$.
  
As far as network generation is concerned we used the standard MCF instance generator \texttt{netgen}\footnote{See Klingman {\em et al.} \cite{netgen}} retrieved at \href{https://github.com/emmanuj/netgen}{https://github.com/emmanuj/netgen} as a C version.
This generator made it possible to vary number of nodes and arc density so that we could test our algorithm over several examples.

Since eigenvalues distribution plays a key role in iterative algorithms' convergence rate and, as stated in the previous section, this holds especially for CG, we deemed the distribution of $D$-eigenvalues as a critical parameter to study experimentally. Trying to figure out a meaningful parametrized distribution to sample from we found that, given the CG convergence rate as a function of $\kappa =  \frac{\lambda_{max}}{\lambda_{min}}$ it would have been nice to have $E^t D^{-1} E$ eigenvalues $0$-symmetrically distributed in logarithmic scale. Although the distribution of $E^t D^{-1} E$ eigenvalues cannot be simply inferred form the one of $D$ (we will see this later in more details) we opted for sampling $D$'s entries exponentiating a r.v. uniformly distributed over $[-rad_D, rad_D]$.  

Finally we needed to generate $b$ such that $\1^t b = 0$ so we just sampled each of $b$'s entries uniformly from an interval $[-rad_b, rad_b]$ and then subtracted from $b$ its projection on $\langle \1 \rangle$. We will see through experiments that $rad_b$ value does not affect our algorithm's performance. 

\subsection{Eigenvalues Distribution}
To grasp some informations about how does $E^t D^{-1} E$ eigenvalues vary while tuning $rad_D$ and the arc density of graph, we plotted the eigenvalues\footnote{Calculated with \texttt{numpy.linalg.eigvals}.} of a $100 \times 100$ matrix induced by a $100$-nodes graph generated as stated above. Please notice that eigenvalues' norm are in logarithmic scale.

\includegraphics[scale=0.5, H]{eigen-100-10-100.png}
\includegraphics[scale=0.5, H]{eigen-100-10-50.png}
\includegraphics[scale=0.5, H]{eigen-100-90-100.png}
\includegraphics[scale=0.5, H]{eigen-100-90-50.png}

As you can see the matrices in the first row, having 10\% arc density, present much more evenly distributed eigenvalues and then we expect CG to have worse convergence on them. As we could easily expect widening up $rad_D$ condition number get worse. We will take this into account later employing a proper precondition while dealing with such matrices.  

\subsection{CG Implementation Performance}
Let us start with a couple of plots showing that our implementation works: the first one took 10.1 \textit{seconds} and achieved a relative error (i.e. $\frac{||Ax - b||_2}{||b||_2}$) of 8.3e-06 while the second took 16.2 \textit{seconds} attaining 8.9e-06 as relative error. 

\includegraphics[scale=0.5, H]{my_cg-500-100000-100.png}
\includegraphics[scale=0.5, H]{my_cg-500-20000-50.png}

It is worth noting that, even if the former graph has both a twice wider $rad_D$ and 5 times more arcs than the latter, CG is clearly faster on the former both in terms of iterations and elapsed time. This is due, of course, to eigenvalues that, as showed above, are much more evenly distributed when the graph is dense. 

\subsection{Comparison with Libraries}
We compared the home-brewed function \texttt{my\_cg} with \texttt{scipy.sparse.linalg.cg}. We found out that our algorithm is slightly better than the scipy's implementation: it took on average 0.14 less iterations and 1.7\% less time to converge achieving a better error on $||Ax - b||$ (6.77e-6 against 6.90e-6). Although not impressive this results shows that1 my implementation works as  expected.

\subsection{Data}
Experiments' data are available, two files in \texttt{.csv} format are attached. The file \texttt{data1} contains data regarding my own implementation while \texttt{data2} contains data produced by the scipy's method. They consist of several rows each one containing 4 columns: (\texttt{rad}, \texttt{itn}, \texttt{t}, \texttt{err}); \texttt{rad} is the radius of the range from which we extract uniformly random values to feed an exponential returning weights, \texttt{itn} is the number of iteration performed, \texttt{t} is the time elapsed and \texttt{err} is the relative error $||Ax - b||$.


\subsection{Convergence Rate}
Here we observe the plot \texttt{rad}-\texttt{itn} showing how CG slow down when the starting matrix is ill conditioned, obtaining a substantial increase in the number of iterations once eigenvalues become sparser and sparser. 



\subsection{Machine}
All the experiment has been run on a an Intel i3-6006U CPU, x86\_64, 2.00GHz running Linux Debian 9.

\section{Code}
The algorithm has been coded in \texttt{python 3.5} exploiting \texttt{scipy} and \texttt{numpy} libraries. Source code is well documented and it has been developed coherently with the \texttt{scipy} style so that \texttt{my\_cg} method has basically the same signature as the off-the-shelf solver and then they may be compared in a few lines of code. All the core functions are in \texttt{cg.py} while \texttt{test.py} contains only methods to automate experimentation.
  
\bibliographystyle{acm}  
\bibliography{biblio}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
