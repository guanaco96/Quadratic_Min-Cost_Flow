\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{geometry}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{amsthm}

\DeclareMathOperator*{\argmin}{arg\,min}
\geometry{a4paper, left=30mm, right=30mm}

\setlength\parindent{0pt}
\newcommand{\R}{\mathbbm{R}}
\newcommand{\1}{\mathbbm{1}}
\newtheorem{theorem}{Theorem}


\title{Conjugate Gradient for \\ Quadratic Minimum-Cost Flow}
\author{Lorenzo Beretta, \texttt{loribere@gmail.com}
  \and Project NoML-13 of C.M. course, C.S. department, UniPi}
\date{3rd July 2019}

\begin{document}
\maketitle


\section{Introduction}
The aim of this project is to exploit the Conjugate Gradient method to solve the
linear system

\begin{equation} \label{linear_system}
  \left(E D^{-1} E^t\right) x = b
\end{equation}

where $E \in \R^{n \times m}$  is the node-edge matrix of a directed graph
and $D \in {\R^{m \times m}$ is a diagonal positive definite matrix.

This problem arises from the KKT conditions of a quadratic separable Minimum-Cost
Flow problem when we do not impose any capacity constraint on arcs.
  
\section{MCF Problem}
Let us derive the equivalence of uncapacited quadratic separable MCF and the linear
system problem stated above.

\subsection{Problem Statement}
Given a directed graph $G = \left(N, A\right)$ such that $|N| = n$ and $|A| = m$ and
a balance vector $b \in \R^n$ such that $\1^t b = 0$ encoding how much
flow is supplied or requested by each node, we want to find a flow vector
$f \in \R^m$ that satisfies $ E f = b $ and minimizes $\frac{1}{2}f^t D f + q^t f$
given a diagonal matrix $D \in \R^{m \times m}$, $D \succeq 0$ and $q \in \R^m$.

\subsection{Optimality as a linear problem}
We can easily reconduct the previous objective function to the simpler
$ \frac{1}{2}\tilde{f}^t D \tilde{f} $, in fact it is sufficient to impose
$\tilde{f} = f - D^{-1} q$, $\tilde{b} = b - E D^{-1} q$ and neglect the
constant offset in the objective function. Hence forgetting tildes our
optimization problem becomes:

\begin{equation}
  \begin{gathered}
    \min_{f \in \R^m} f^t D f \\
    E f = b
  \end{gathered}
\end{equation}

Then imposing Lagrangian multipliers conditions (a.k.a. ``poorman'' KKT) we get

\begin{equation*}
  f \text{ is optimal} \iff
  \left\{\begin{gathered}
    f^t D = x^t E \\  
    E f = b
  \end{gatheres}
  \iff
   \left\{\begin{gathered}
    f = D^{-1} E^t x\\  
    \left(E D^{-1} E^t\right) x = b
  \end{gatheres}
\end{equation*}

From now on we can forget the optimization problem and just deal with the linear system.

\section{Conjugate Gradient Algorithm}
In this section we will show the conjugate gradient method to solve a linear  system of the form $ A x = b $ for a symmetric and positive semidefinite matrix $A$.

Our main reference for this method is the book of Trefethen {\em et al.} \cite{trefethen97} that treats the strictly positive definite case, it is easy to adapt their proofs to show that assuming $b \in range(A)$ all results stated there hold even in the semidefinite case. This is necessary to our application since $E^t D^{-1} E$ has rank exactly $n-1$ in fact $rk(E)=n-c$ where $c$ is the number of connected components of the graph (assumed to be connected) and $rk(D) = m$.
Of course the algorithm converges to one of the infinite solutions of the system if $rk(A) < n$ and in particular it converges to $x_*$ such that $\1^t x_* = 0$ where $Ker(E^t D^{-1} E) = \langle \1 \rangle$.

\subsection{Algorithm}
In the following algorithm $x_n$ is the approximate solution at step $n$ and the cycle is actually repeated until a suitable convergence error is achieved.

\vbox{
\begin{algorithmic}[1]
\Procedure{ConjugateGradient}{$A, b$}
  \State {$ x_0 = 0, \quad r_0 = b, \quad p_0 = b $}
  \For {$ n = 1,\, 2,\, 3,\, \dots $}
    \State {$ \alpha_n = \left(r_ {n-1}^tr_ {n-1}\right) / \left(p_ {n-1}^tAp_ {n-1}}\right) $} \Comment{Step Length}
    \State {$ x_n = x_{n-1} + \alpha_n p_{n-1} $} \Comment{Approximate Solution}
    \State {$ r_n = r_{n-1} - \alpha_n A p_{n-1} $} \Comment{Residual}
    \State {$\beta_n = \left(r_n^t r_n\right) / \left(r_{n-1}^t r_{n-1}^t\right)$} \Comment{Improvement this Step}
    \State {$p_n = r_n + \beta_n p_{n-1} $} \Comment{Search Direction}
  \EndFor
\EndProcedure
\end{algorithmic}
}
\hspace{1cm}

It is worth noting that the bottleneck of the single iteration is the computation of $Ap_{n-1}$ and it can be fasten up considering the structure of A. It suffices to notice that $E$ has exactly $2m$ nonzero elements and employ a sparse matrix multiplication to achieve an $O(m)$ complexity per iteration. Moreover our implementation performs exactly $m + O(1)$ multiplications and $3m + O(1)$ additions keeping the constant factor very low.

\subsection{CG as a Direct Method}
The main property of this algorithm is that for every $n$ such that the algorithm has not converged yet it holds that:

\begin{equation*}
  \mathcal{K}_n = \langle x_1,\, \dots,\,x_n\rangle = \langle r_0,\, \dots,\,r_{n-1}\rangle = \langle p_0,\, \dots,\,p_{n-1}\rangle = \langle b,\,A b, \, \dots,\,A^{n-1} b\rangle \\
\end{equation*}

\begin{equation*}
  r_n^t r_j = 0, \quad p_n^t A p_j = 0 \quad \forall j < n
\end{equation*}
\hspace{0.5cm}

It is easy to show that this property entails that the algorithm is well defined even in the singular case since $ b \in range(A) \implies p_{n-1} \in range(A)$. Moreover the fundamental corollary of that property is that  at each step
$$x_n = \argmin_{z \in \mathcal{K}_n} || x_* - z ||_A $$
for every $x_*$ such that $Ax_* = b$. Therefore in exact arithmetic it must converge in at most $n$ steps and the method is then a direct one.

\subsection{CG as an Interative Method}
Conjugate gradient has a twofold nature and can be interpreted as an optimization algorithm whose objective function is $ || x_* - x ||_A $ that up to a positive scaling and a shift is equal to $ \frac{1}{2}x^t A x - x^t b$. Then it becomes interesting to study its convergence rate trying to speedup the direct method bound of $n$ iterations.

The main result employed to derive the convergence rate is the a slight modification of theorem 38.3 of Trefethen {\em et al.} \cite{trefethen97}:

\begin{theorem}[Polynomial Approximation]\label{polap}
  Let us define $e_n = x_* - x_n$ and $P_n = \left\{p \in \R[x]\, |\, deg(p) \leq n, \; p(0) = 1 \right\}$. If CG is not converged yet before step $n$ (i.e. $r_{n-1} \neq 0$) then 
$$ \frac{||e_n||_A}{||e_0||_A} \leq \inf_{p \in P_n}\frac{||p(A) e_0||_A}{||e_0||_A} \leq \inf_{p \in P_n} \max_{\lambda \in sp(A)\setminus\{0\}}|p(\lambda)|
$$
\end{theorem}
\begin{proof}
  Let us decompose $e_0$ over the orthonormal basis of $A$-eigenvectors as\\ $e_0 = \sum_i a_i v_i$ provided that $A v_i = \lambda_i v_i$, then
$$\inf_{p \in P_n}\frac{||p(A) e_0||_A}{||e_0||_A} \leq \inf_{p \in P_n}\sqrt{\frac{\sum_i a_i^2 \lambda_i \left(p(\lambda_i)\right)^2}{\sum_i a_i^2 \lambda_i}}\leq \inf_{p \in P_n} \max_{\lambda \in sp(A)\setminus\{0\}}|p(\lambda)|$$

\end{proof}

 Theorem \ref{polap} enhances theorem 38.3 of Trefethen {\em et al.} \cite{trefethen97} taking into account the singular case and showing that even in that case this bound doesn't degrade.

\subsection{Rate of Convergence}
The results above provide a better although trivial estimate for the ``exact method'' in fact if $A$ has $h$ nonzero different eigenvalues then after $h$ iterations RHS in the previous inequality is null since the polynomial perfectly interpolates eigenvalues.

On the other hand a more technical result involving the iterative nature of the algorithm can be proven: given the condition number $\kappa = \frac{\lambda_{max}}{\lambda_{min}}$ of $A$ it holds that

$$ \frac{||e_n||_A}{||e_0||_A} \leq 2 \left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^n $$

Finally we may say that a ``good'' spectrum for $A$ (i.e. providing fast convergence of CG) is one that has at least one of the two: (i) eigenvalue forms small clusters so that polynomials can interpolate them ``all at once'' or (ii) eigenvalues are not too sparse in logarithmic scale. We are now confirming this trend experimentally. 

\section{Experiments}
\subsection{Data Generation}
In order to perform the experiments we needed to generate an undirected graph and a weight for each arc (representing the quadratic coefficient). As far as network generation is concerned we used the well-known capacited network generator NetGen retrieved at \href{http://www.di.unipi.it/optimize/Data/MCF.html}{http://www.di.unipi.it/optimize/Data/MCF.html}. All the experiments has been performed on graphs having 3000 arcs and density 0.75. In order to generate parameters we used a uniform distributions and exponentiate them since we were interested in sparsity in logarithmic scale. Further we varied the range in within we sampled uniformly showing that the problem becomes quickly intractable if eigenvalues are (relatively) too close to the origin.

\subsection{Comparison with Libraries}
We compared the home-brewed function \texttt{my\_cg} with \texttt{scipy.sparse.linalg.cg}. We found out that our algorithm is slightly better than the scipy's implementation: it took on average 0.14 less iterations and 1.7\% less time to converge achieving a better error on $||Ax - b||$ (6.77e-6 against 6.90e-6). Although not impressive this results shows that my implementation works as expected.

\subsection{Data}
Experiments' data are available, two files in \texttt{.csv} format are attached. The file \texttt{data1} contains data regarding my own implementation while \texttt{data2} contains data produced by the scipy's method. They consist of several rows each one containing 4 columns: (\texttt{rad}, \texttt{itn}, \texttt{t}, \texttt{err}); \texttt{rad} is the radius of the range from which we extract uniformly random values to feed an exponential returning weights, \texttt{itn} is the number of iteration performed, \texttt{t} is the time elapsed and \texttt{err} is the relative error $||Ax - b||$.


\subsection{Convergence Rate}
Here we observe the plot \texttt{rad}-\texttt{itn} showing how CG slow down when the starting matrix is ill conditioned, obtaining a substantial increase in the number of iterations once eigenvalues become sparser and sparser. 

\begin{center}
\includegraphics[scale=0.7, H]{rad-itn.png}
\end{center}

\subsection{Machine}
All the experiment has been run on a an Intel i3-6006U CPU, x86\_64, 2.00GHz running Linux Debian 9.

\section{Code}
The algorithm has been coded in \texttt{python 3.5} exploiting \texttt{scipy} and \texttt{numpy} libraries. Source code is well documented and it has been developed coherently with the \texttt{scipy} style so that \texttt{my\_cg} method has basically the same signature as the off-the-shelf solver and then they may be compared in a few lines of code. All the core functions are in \texttt{cg.py} while \texttt{test.py} contains only methods to automate experimentation.
  
\bibliographystyle{acm}  
\bibliography{biblio}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
